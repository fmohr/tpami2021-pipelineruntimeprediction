
- Why are you focused only on two-step classification problems?  What's wrong with regression?  Should you make any difference in the proposed approach? You might want to include some justifications/explanations.
Good point, the approach is completely compatible for regression problems. We just did not include any remark on this in the introduction or other parts of the paper, because there are reviewers who take such a statement as an "undelivered promise" and reason to reject a paper if it is not sustained in the evaluation. Since we have not the space to cover two different types of algorithms in the main body of the paper, we even omitted this. However, we agree that this should be mentioned and, taking your affirmative remark, we included a respective sentence in the introduction and section 2.

- I wonder how you would integrate your runtime prediction method on an optimisation approach that has a global timeout to find a pipeline (e.g. Auto ScikitLearn), but the method explores multiple pipelines at once. Your solution would say if it ‘fits’ within the given time, but in conjunction with other pipeline evaluations there might be other decision to make! I would simply like to know the opinion of the authors about this.
 First we would like to note that ML-Plan *is* parallelized, and the experiments are also conducted with parallel executions. This being said, in this paper we assume that a deadline is given not only overall but also for the concrete single evaluations. In this case, parallelism does not affect the approach, because our predictor is just queried immediately before execution (no matter whether other pipelines are evaluated in parallel). However, it is indeed an interesting idea to use a predictor like our to *get rid* of the timeout parameter for single evaluations, which is anyways hard to specify for the user. Going into this direction, the decision should be taken with the *expected performance* of the pipeline in mind, trading runtime cost against the expected benefits of the evaluation. This would hold for both sequential and parallel systems.
 
 - I think the paper could benefit from a global flowchart/workflow figure that show how the regression model is actually created…i.e.  from a dataset, and pipeline, how do you create the input data for your regression model, and how you ‘test’ it. A global view of the method that considers both the acquisition of data for preprocessing techniques and learning.
 We added a corresponding figure
 
 -In Section 5.2.1. It is unclear why the authors chose those meta features for the datasets… why not others? Whilst I agree that the less feature the easier the training, this may also play a role in generalising to datasets you haven’t seen before?  I was thinking for example about pre-processing techniques that may require more time depending on the distribution of the classes.
 Class distribution could be indeed interesting, we did not exclude that specific feature for a particular reason. Our goal is to obtain high prediction quality while using as few features as possible. Section 5.3 shows that the simple features enable a quite good predictability. In the supplementary material we show that more sophisticated feature sets did not bring better results.     
 
 - The generated database of runtimes is very impressive. I wonder if the authors are planning to release that publicly for future research.  It would also be nice to have some summary of properties (stats) of the generated databases before training the regression models. For example, number of features, type of features?, number of instances, etc.
 Yes, we will provide the results to the community. Given the size of the database, the eventual form is not entirely clear yet, but the runtime data will be made available definetly.
 With respect to your second remark, we included the statistical information into the respective sections <which ones?>.