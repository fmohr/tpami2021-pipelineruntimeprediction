{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commons import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openmlids = [41066, 41065, 41064, 3, 31, 60, 40685, 183, 1501, 554, 1457, 40691, 40975, 4136, 4137, 41026, 1485, 40926, 40927, 181, 40497, 23512, 1486, 41146, 41143, 1037, 1039, 1242, 1216, 1042, 1569, 4541, 4552, 23380, 273, 41991, 1142, 1146, 1134, 1138, 1139, 1128, 1130, 1112, 1114, 1161, 1166, 41164, 40900, 41946, 40971]\n",
    "#print(sorted(l))\n",
    "openmlids = [3, 6, 12, 14, 16, 18, 21, 22, 23, 24, 26, 28, 30, 31, 32, 36, 38, 44, 46, 57, 60, 179, 180, 181, 182, 183, 184, 185, 273, 293, 300, 351, 354, 357, 389, 390, 391, 392, 393, 395, 396, 398, 399, 401, 554, 679, 715, 718, 720, 722, 723, 727, 728, 734, 735, 737, 740, 741, 743, 751, 752, 761, 772, 797, 799, 803, 806, 807, 813, 816, 819, 821, 822, 823, 833, 837, 843, 845, 846, 847, 849, 866, 871, 881, 897, 901, 903, 904, 910, 912, 913, 914, 917, 923, 930, 934, 953, 958, 959, 962, 966, 971, 976, 977, 978, 979, 980, 991, 993, 995, 1000, 1002, 1018, 1019, 1020, 1021, 1036, 1037, 1039, 1040, 1041, 1042, 1049, 1050, 1053, 1059, 1067, 1068, 1069, 1111, 1112, 1114, 1116, 1119, 1120, 1128, 1130, 1134, 1138, 1139, 1142, 1146, 1161, 1166, 1216, 1242, 1457, 1485, 1486, 1501, 1569, 4136, 4137, 4541, 4552, 23380, 23512, 40497, 40685, 40691, 40900, 40926, 40927, 40971, 40975, 41026, 41064, 41065, 41066, 41143, 41146, 41164, 41946, 41991]\n",
    "trainsizes = [2, 4, 8, 16, 50, 75, 100, 125, 150, 200, 250, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 60000, 70000, 80000, 90000, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "datasets = pd.DataFrame(np.array([[3, 3196],\n",
    "\t\t[6, 20000],\n",
    "\t\t[12, 2000],\n",
    "\t\t[14, 2000],\n",
    "\t\t[16, 2000],\n",
    "\t\t[18, 2000],\n",
    "\t\t[21, 1728],\n",
    "\t\t[22, 2000],\n",
    "\t\t[23, 1473],\n",
    "\t\t[24, 8124],\n",
    "\t\t[26, 12960],\n",
    "\t\t[28, 5620],\n",
    "\t\t[30, 5473],\n",
    "\t\t[31, 1000],\n",
    "\t\t[32, 10992],\n",
    "\t\t[36, 2310],\n",
    "\t\t[38, 3772],\n",
    "\t\t[44, 4601],\n",
    "\t\t[46, 3190],\n",
    "\t\t[57, 3772],\n",
    "\t\t[60, 5000],\n",
    "\t\t[179, 48842],\n",
    "\t\t[180, 110393],\n",
    "\t\t[181, 1484],\n",
    "\t\t[182, 6430],\n",
    "\t\t[183, 4177],\n",
    "\t\t[184, 28056],\n",
    "\t\t[185, 1340],\n",
    "\t\t[273, 120919],\n",
    "\t\t[293, 581012],\n",
    "\t\t[300, 7797],\n",
    "\t\t[351, 488565],\n",
    "\t\t[354, 1025010],\n",
    "\t\t[357, 98528],\n",
    "\t\t[389, 2463],\n",
    "\t\t[390, 9558],\n",
    "\t\t[391, 1504],\n",
    "\t\t[392, 1003],\n",
    "\t\t[393, 3075],\n",
    "\t\t[395, 1657],\n",
    "\t\t[396, 3204],\n",
    "\t\t[398, 1560],\n",
    "\t\t[399, 11162],\n",
    "\t\t[401, 1050],\n",
    "\t\t[554, 70000],\n",
    "\t\t[679, 1024],\n",
    "\t\t[715, 1000],\n",
    "\t\t[718, 1000],\n",
    "\t\t[720, 4177],\n",
    "\t\t[722, 15000],\n",
    "\t\t[723, 1000],\n",
    "\t\t[727, 40768],\n",
    "\t\t[728, 4052],\n",
    "\t\t[734, 13750],\n",
    "\t\t[735, 8192],\n",
    "\t\t[737, 3107],\n",
    "\t\t[740, 1000],\n",
    "\t\t[741, 1024],\n",
    "\t\t[743, 1000],\n",
    "\t\t[751, 1000],\n",
    "\t\t[752, 8192],\n",
    "\t\t[761, 8192],\n",
    "\t\t[772, 2178],\n",
    "\t\t[797, 1000],\n",
    "\t\t[799, 1000],\n",
    "\t\t[803, 7129],\n",
    "\t\t[806, 1000],\n",
    "\t\t[807, 8192],\n",
    "\t\t[813, 1000],\n",
    "\t\t[816, 8192],\n",
    "\t\t[819, 9517],\n",
    "\t\t[821, 22784],\n",
    "\t\t[822, 20640],\n",
    "\t\t[823, 20640],\n",
    "\t\t[833, 8192],\n",
    "\t\t[837, 1000],\n",
    "\t\t[843, 22784],\n",
    "\t\t[845, 1000],\n",
    "\t\t[846, 16599],\n",
    "\t\t[847, 6574],\n",
    "\t\t[849, 1000],\n",
    "\t\t[866, 1000],\n",
    "\t\t[871, 3848],\n",
    "\t\t[881, 40768],\n",
    "\t\t[897, 1161],\n",
    "\t\t[901, 40768],\n",
    "\t\t[903, 1000],\n",
    "\t\t[904, 1000],\n",
    "\t\t[910, 1000],\n",
    "\t\t[912, 1000],\n",
    "\t\t[913, 1000],\n",
    "\t\t[914, 2001],\n",
    "\t\t[917, 1000],\n",
    "\t\t[923, 8641],\n",
    "\t\t[930, 1302],\n",
    "\t\t[934, 1156],\n",
    "\t\t[953, 3190],\n",
    "\t\t[958, 2310],\n",
    "\t\t[959, 12960],\n",
    "\t\t[962, 2000],\n",
    "\t\t[966, 1340],\n",
    "\t\t[971, 2000],\n",
    "\t\t[976, 9961],\n",
    "\t\t[977, 20000],\n",
    "\t\t[978, 2000],\n",
    "\t\t[979, 5000],\n",
    "\t\t[980, 5620],\n",
    "\t\t[991, 1728],\n",
    "\t\t[993, 7019],\n",
    "\t\t[995, 2000],\n",
    "\t\t[1000, 3772],\n",
    "\t\t[1002, 7485],\n",
    "\t\t[1018, 8844],\n",
    "\t\t[1019, 10992],\n",
    "\t\t[1020, 2000],\n",
    "\t\t[1021, 5473],\n",
    "\t\t[1036, 14395],\n",
    "\t\t[1037, 4562],\n",
    "\t\t[1039, 4229],\n",
    "\t\t[1040, 14395],\n",
    "\t\t[1041, 3468],\n",
    "\t\t[1042, 3468],\n",
    "\t\t[1049, 1458],\n",
    "\t\t[1050, 1563],\n",
    "\t\t[1053, 10885],\n",
    "\t\t[1059, 121],\n",
    "\t\t[1067, 2109],\n",
    "\t\t[1068, 1109],\n",
    "\t\t[1069, 5589],\n",
    "\t\t[1111, 50000],\n",
    "\t\t[1112, 50000],\n",
    "\t\t[1114, 50000],\n",
    "\t\t[1116, 6598],\n",
    "\t\t[1119, 32561],\n",
    "\t\t[1120, 19020],\n",
    "\t\t[1128, 1545],\n",
    "\t\t[1130, 1545],\n",
    "\t\t[1134, 1545],\n",
    "\t\t[1138, 1545],\n",
    "\t\t[1139, 1545],\n",
    "\t\t[1142, 1545],\n",
    "\t\t[1146, 1545],\n",
    "\t\t[1161, 1545],\n",
    "\t\t[1166, 1545],\n",
    "\t\t[1216, 1496391],\n",
    "\t\t[1242, 98528],\n",
    "\t\t[1457, 1500],\n",
    "\t\t[1485, 2600],\n",
    "\t\t[1486, 34465],\n",
    "\t\t[1501, 1593],\n",
    "\t\t[1569, 1025000],\n",
    "\t\t[4136, 600],\n",
    "\t\t[4137, 1150],\n",
    "\t\t[4541, 101766],\n",
    "\t\t[4552, 5665],\n",
    "\t\t[23380, 2796],\n",
    "\t\t[23512, 98050],\n",
    "\t\t[40497, 3772],\n",
    "\t\t[40594, 2000],\n",
    "\t\t[40685, 58000],\n",
    "\t\t[40691, 1599],\n",
    "\t\t[40900, 5100],\n",
    "\t\t[40926, 20000],\n",
    "\t\t[40927, 60000],\n",
    "\t\t[40971, 1000],\n",
    "\t\t[40975, 1728],\n",
    "\t\t[41026, 7000],\n",
    "\t\t[41064, 58000],\n",
    "\t\t[41065, 62000],\n",
    "\t\t[41066, 1567],\n",
    "\t\t[41143, 2984],\n",
    "\t\t[41146, 5124],\n",
    "\t\t[41164, 8237],\n",
    "\t\t[41946, 3772],\n",
    "\t\t[41991, 270912]]), columns=[\"openmlid\", \"totalsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to_csv() got an unexpected keyword argument 'delimiter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0836f63cb3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/rawruntimes/runtimes_classifiers_parametrized_out.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"experiment_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"openmlid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fitsize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"algorithm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"algorithmoptions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fittime\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"applicationtime\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exception\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/rawruntimes/runtimes_classifiers_parametrized.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: to_csv() got an unexpected keyword argument 'delimiter'"
     ]
    }
   ],
   "source": [
    "fin = open(\"data/rawruntimes/runtimes_classifiers_parametrized2.csv\", \"rt\")\n",
    "fout = open(\"data/rawruntimes/runtimes_classifiers_parametrized_out.csv\", \"wt\")\n",
    "\n",
    "for line in fin:\n",
    "\tfout.write(line.replace('\\\\N', ''))\n",
    "\t\n",
    "fin.close()\n",
    "fout.close()\n",
    "df = pd.read_csv(\"data/rawruntimes/runtimes_classifiers_parametrized_out.csv\", delimiter=\";\", names=[\"experiment_id\", \"openmlid\", \"fitsize\", \"algorithm\", \"algorithmoptions\", \"seed\", \"fittime\", \"applicationtime\", \"exception\"])\n",
    "df.to_csv(\"data/rawruntimes/runtimes_classifiers_parametrized.csv\", delimiter=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea77075ea3e84529954289de9eb7cef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45750), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>openmlid</th>\n",
       "      <th>fitsize</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>41066</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>41066</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41065</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>41066</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>41064</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1657</td>\n",
       "      <td>1114</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1658</td>\n",
       "      <td>1112</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1659</td>\n",
       "      <td>1112</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1114</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1661</td>\n",
       "      <td>1112</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1662 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     openmlid fitsize seed\n",
       "0       41066       1    0\n",
       "1       41066       1    1\n",
       "2       41065       1    0\n",
       "3       41066       1    2\n",
       "4       41064       1    0\n",
       "...       ...     ...  ...\n",
       "1657     1114       2    7\n",
       "1658     1112       2    8\n",
       "1659     1112       2    9\n",
       "1660     1114     500    5\n",
       "1661     1112       2    4\n",
       "\n",
       "[1662 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getRuntimesObservationsForWhichNoMetaFeaturesAreKnown():\n",
    "    rAtts = [\"openmlid\", \"fitsize\", \"seed\"]\n",
    "    mDF = pd.DataFrame([], columns=rAtts)\n",
    "    dfMetafeatures = pd.read_csv(\"data/metafeatures.csv\", delimiter=\";\")\n",
    "    dfClassifiers = pd.read_csv(\"data/rawruntimes/runtimes_classifiers_default.csv\", delimiter=\";\")\n",
    "    dfClassifiers = dfClassifiers[dfClassifiers[\"exception\"].isnull()][rAtts].drop_duplicates()\n",
    "    datasets = pd.unique(dfClassifiers[\"openmlid\"])\n",
    "\n",
    "    pbar = tqdm(total=len(dfClassifiers))\n",
    "    for index, obs in dfClassifiers.iterrows():\n",
    "        pbar.update(1)\n",
    "        if len(dfMetafeatures.query(\"openmlid == \" + str(obs[\"openmlid\"]) + \" and datapoints_fold1 == \" + str(obs[\"fitsize\"]) + \" and seed == \" + str(obs[\"seed\"]))) == 0:\n",
    "            mDF.loc[len(mDF)] = obs[rAtts]\n",
    "    pbar.close()\n",
    "    return mDF\n",
    "mDF = getRuntimesObservationsForWhichNoMetaFeaturesAreKnown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolveExceptions(df):\n",
    "    # Set traintime to full traintime +1 (3601) if the exception is a timeout or is dominated by another one that timed out\n",
    "    timeouts = []\n",
    "    failedRuns = df[df[\"exception\"].notnull()]\n",
    "    failedRunsWithPositiveTime = failedRuns.query(\"fittime >= 0\")\n",
    "    pbar = tqdm(total=len(failedRunsWithPositiveTime))\n",
    "    for i, row in failedRunsWithPositiveTime.iterrows():\n",
    "        pbar.update(1)\n",
    "        exception = str(row[\"exception\"])\n",
    "        if \"Timeout\" in exception and row[\"fittime\"] >= 3600:\n",
    "            df.at[i, \"exception\"] = np.nan\n",
    "            timeouts.append((row[\"openmlid\"], row[\"algorithm\"], row[\"fitsize\"]))\n",
    "    pbar.close()\n",
    "    print(\"Found \" + str(len(timeouts)) + \" timeouts.\")\n",
    "    pbar = tqdm(total=len(failedRuns))\n",
    "    for i, row in failedRuns.iterrows():\n",
    "        exception = str(row[\"exception\"])\n",
    "        pbar.update(1)\n",
    "        if \"canceled due to\" in exception:\n",
    "            t = (row[\"openmlid\"], row[\"algorithm\"], row[\"fitsize\"])\n",
    "            isDominatedDueToTimeout = False\n",
    "            for t2 in timeouts:\n",
    "                # if the datapoint is dominated by another one due to a timeout, set the traintime to that time\n",
    "                 if t2[0] == t[0] and t2[1] == t[1] and t2[2] < t[2]:\n",
    "                    df.at[i, \"fittime\"] = 3601\n",
    "                    df.at[i, \"exception\"] = np.nan\n",
    "                    break\n",
    "    pbar.close()\n",
    "    \n",
    "    print(\"Removing lines with exceptions.\")\n",
    "    df = df[df[\"exception\"].isnull()]\n",
    "    df = df.drop(columns=[\"exception\"])\n",
    "    \n",
    "    print(\"Now removing lines where fittime is not available.\")\n",
    "    df = df[df[\"fittime\"].notnull()]\n",
    "    return df\n",
    "\n",
    "## Step 1: merge classifier and pre-processor results into one huge runtime dataset\n",
    "def getAllRuntimesWithMetafeaturesForDefault():\n",
    "    \n",
    "    # read in basic datasets\n",
    "    print(\"Read in data\")\n",
    "    dfMetafeatures = pd.read_csv(\"data/metafeatures.csv\", delimiter=\";\")\n",
    "    dfClassifiers = pd.read_csv(\"data/rawruntimes/runtimes_classifiers_default.csv\", delimiter=\";\")\n",
    "    df = dfClassifiers.merge(dfMetafeatures, left_on=[\"openmlid\", \"fitsize\", \"seed\"], right_on=[\"openmlid\", \"datapoints_fold1\", \"seed\"])\n",
    "    df = df.merge(datasets, on=[\"openmlid\"])\n",
    "    print(str(len(df)) + \"/\" + str(len(dfClassifiers)))\n",
    "    df[\"applicationsize\"] = df[\"totalsize\"] - df[\"fitsize\"]\n",
    "    df = df.rename(columns={\"f1_numericattributesafterbinarization\": \"fitattributes\"})\n",
    "    df = df[['openmlid', 'totalsize', 'fitsize', 'applicationsize', 'fitattributes', 'seed', 'algorithm', 'fittime', 'applicationtime', 'exception']]\n",
    "    pdf = pd.read_csv(\"data/rawruntimes/runtimes_preprocessors_default_out.csv\", delimiter=\";\", index_col=None)\n",
    "    pdf = pdf.rename(columns={\"datapoints\": \"fitsize\", \"numericattributesafterbinarization_before\": \"fitattributes\"})\n",
    "    pdf = pdf[['openmlid', 'fitsize', 'fitattributes', 'seed', 'algorithm', 'fittime', 'applicationtime', 'exception']]\n",
    "    pdf = pdf.merge(datasets, on=[\"openmlid\"])\n",
    "    pdf[\"applicationsize\"] = pdf[\"totalsize\"] - pdf[\"fitsize\"]\n",
    "    pdf[~pdf[\"fittime\"].isnull()]\n",
    "    pdf = pdf[['openmlid', 'totalsize', 'fitsize', 'applicationsize', 'fitattributes', 'seed', 'algorithm', 'fittime', 'applicationtime', 'exception']]\n",
    "    df = pd.concat([df, pdf])\n",
    "    print(\"Basic data read successfully. Now replacing exception entries due to dominance by runtime 3601.\")\n",
    "    \n",
    "    df = resolveExceptions(df)\n",
    "\n",
    "    print(\"Now removing lines where fitattributes is not available.\")\n",
    "    df = df[df[\"fitattributes\"].notnull()]\n",
    "    print(\"Ready. Prepared clean dataset.\")\n",
    "    return df.astype({\"fitattributes\": int, \"fittime\": int})\n",
    "\n",
    "## Step 1: merge classifier and pre-processor results into one huge runtime dataset\n",
    "def getAllRuntimesWithMetafeaturesForParametrized():\n",
    "    \n",
    "    # read in basic datasets\n",
    "    print(\"Read in data\")\n",
    "    dfMetafeatures = pd.read_csv(\"data/metafeatures.csv\", delimiter=\";\")\n",
    "    dfClassifiers = pd.read_csv(\"data/rawruntimes/runtimes_classifiers_parametrized.csv\", delimiter=\";\")\n",
    "    df = dfClassifiers.merge(dfMetafeatures, left_on=[\"openmlid\", \"fitsize\", \"seed\"], right_on=[\"openmlid\", \"datapoints_fold1\", \"seed\"])\n",
    "    df = df.merge(datasets, on=[\"openmlid\"])\n",
    "    print(str(len(df)) + \"/\" + str(len(dfClassifiers)))\n",
    "    df[\"applicationsize\"] = df[\"totalsize\"] - df[\"fitsize\"]\n",
    "    df = df.rename(columns={\"f1_numericattributesafterbinarization\": \"fitattributes\"})\n",
    "    df = df[['openmlid', 'totalsize', 'fitsize', 'applicationsize', 'fitattributes', 'seed', 'algorithm', 'algorithmoptions', 'fittime', 'applicationtime', 'exception']]\n",
    "    print(\"Basic data read successfully. Now replacing exception entries due to dominance by runtime 3601.\")\n",
    "    \n",
    "    df = resolveExceptions(df)\n",
    "\n",
    "    print(\"Now removing lines where fitattributes is not available.\")\n",
    "    df = df[df[\"fitattributes\"].notnull()]\n",
    "    print(\"Ready. Prepared clean dataset.\")\n",
    "    return df.astype({\"fitattributes\": int, \"fittime\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in data\n",
      "148974/180327\n",
      "Basic data read successfully. Now replacing exception entries due to dominance by runtime 3601.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de61cac74b543d588e40dba94652adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28432), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 4370 timeouts.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957b9794a03748bfbc7bdeef8ad7d3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28432), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing lines with exceptions.\n",
      "Now removing lines where fittime is not available.\n",
      "Now removing lines where fitattributes is not available.\n",
      "Ready. Prepared clean dataset.\n"
     ]
    }
   ],
   "source": [
    "df = getAllRuntimesWithMetafeaturesForParametrized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/runtimes_all_default.csv\").astype({\"fitattributes\": int, \"fittime\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"algorithm == 'ann'\")\n",
    "df.to_csv(\"data/runtimes_all_parametrized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropOutliers(df, batch):\n",
    "    if len(batch) >= 4:\n",
    "        q3 = np.quantile(batch[\"fittime\"], .75)\n",
    "        outliers = batch.query(\"fittime > \" + str(q3 * 10))\n",
    "        if len(outliers) > 0:\n",
    "            #print(batch)\n",
    "            #print(df.iloc[outliers.index])\n",
    "            df = df.drop(index = outliers.index)\n",
    "    return df\n",
    "\n",
    "def removeOutliersFromBatches(df):\n",
    "    print(\"Size before: \" + str(len(df)))\n",
    "    algorithms = pd.unique(df[\"algorithm\"])\n",
    "    datasets = pd.unique(df[\"openmlid\"])\n",
    "    pbar = tqdm(total = len(algorithms) * len(datasets))\n",
    "    for c in algorithms:\n",
    "        cDF = df.query(\"algorithm == '\" + c + \"'\")\n",
    "        numDatasets = datasets.shape[0]\n",
    "        for d in datasets:\n",
    "            dDF = cDF.query(\"openmlid == \" + str(d))\n",
    "            fitsizes = pd.unique(dDF[\"fitsize\"])\n",
    "            fitattributes = pd.unique(dDF[\"fitattributes\"])\n",
    "            for size in fitsizes:\n",
    "                for atts in fitattributes:\n",
    "                    rDF = dDF.query(\"fitsize == \" + str(size) + \" and fitattributes == \" + str(atts))\n",
    "                    if \"algorithmoptions\" in df.columns:\n",
    "                        options = pd.unique(rDF[\"algorithmoptions\"])\n",
    "                        for o in options:\n",
    "                            df = dropOutliers(df, rDF.query(\"algorithmoptions == '\" + str(o) + \"'\"))\n",
    "                    else:\n",
    "                        df = dropOutliers(df, rDF)\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(\"Size after outlier removal: \" + str(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before: 1017237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b14ff968014863803c9ae43485ee0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size after outlier removal: 1000006\n"
     ]
    }
   ],
   "source": [
    "dfNoOutliers = removeOutliersFromBatches(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoOutliers.to_csv(\"data/runtimes_all_default_nooutliers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = pd.read_csv(\"preprocessor_runtimes_dup.csv\").merge(datasets, on='openmlid')\n",
    "df[\"testpoints\"] = df[\"datapoints_x\"] - df[\"datapoints_y\"]\n",
    "#df = df[[\"openmlid\", \"datapoints\", \"trainpoints\", \"testpoints\", \"seed\", \"classifier\", \"traintime\", \"testtime\", \"exception\"]]\n",
    "#df.to_csv(\"preprocessor_runtimes_dup2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['openmlid', 'algo', 'datapoints_y', 'datapoints_x', \n",
    "       'testpoints', 'attributes', 'seed',\n",
    "       'numinstances_before', 'numattributes_before',\n",
    "       'numlabels_before', 'numnumericattributes_before',\n",
    "       'numsymbolicattributes_before', 'numberofcategories_before',\n",
    "       'numericattributesafterbinarization_before',\n",
    "       'attributestocover50pctvariance_before',\n",
    "       'attributestocover90pctvariance_before',\n",
    "       'attributestocover95pctvariance_before',\n",
    "       'attributestocover99pctvariance_before', 'accvariancerel1_before',\n",
    "       'accvariancerel2_before', 'accvariancerel3_before',\n",
    "       'accvariancerel4_before', 'accvariancerel5_before',\n",
    "       'accvariancerel6_before', 'accvariancerel7_before',\n",
    "       'accvariancerel8_before', 'accvariancerel9_before',\n",
    "       'accvariancerel10_before', 'accvarianceabs1_before',\n",
    "       'accvarianceabs2_before', 'accvarianceabs3_before',\n",
    "       'accvarianceabs4_before', 'accvarianceabs5_before',\n",
    "       'accvarianceabs6_before', 'accvarianceabs7_before',\n",
    "       'accvarianceabs8_before', 'accvarianceabs9_before',\n",
    "       'accvarianceabs10_before', 'totalvariance_before', 'numinstances_after',\n",
    "       'numattributes_after', 'numlabels_after', 'numnumericattributes_after',\n",
    "       'numsymbolicattributes_after', 'numberofcategories_after',\n",
    "       'numericattributesafterbinarization_after',\n",
    "       'attributestocover50pctvariance_after',\n",
    "       'attributestocover90pctvariance_after',\n",
    "       'attributestocover95pctvariance_after',\n",
    "       'attributestocover99pctvariance_after', 'accvariancerel1_after',\n",
    "       'accvariancerel2_after', 'accvariancerel3_after',\n",
    "       'accvariancerel4_after', 'accvariancerel5_after',\n",
    "       'accvariancerel6_after', 'accvariancerel7_after',\n",
    "       'accvariancerel8_after', 'accvariancerel9_after',\n",
    "       'accvariancerel10_after', 'accvarianceabs1_after',\n",
    "       'accvarianceabs2_after', 'accvarianceabs3_after',\n",
    "       'accvarianceabs4_after', 'accvarianceabs5_after',\n",
    "       'accvarianceabs6_after', 'accvarianceabs7_after',\n",
    "       'accvarianceabs8_after', 'accvarianceabs9_after',\n",
    "       'accvarianceabs10_after', 'totalvariance_after', 'memory_peak',\n",
    "       'traintime', 'applicationtime', 'exception']].rename(columns={\"datapoints_x\": \"trainpoints\", \"datapoints_y\": \"datapoints\", \"testpoints\": \"applicationpoints\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"preprocessor_runtimes_dup2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
